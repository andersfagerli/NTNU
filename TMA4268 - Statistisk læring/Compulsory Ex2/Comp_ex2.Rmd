subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 2"
author: "Anders Fagerli, Rebecca Sandstø"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
```

# Problem 1: Regression [6 points]

```{r}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
```
***
**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

```{r,eval = TRUE}
full = lm(price ~. -logprice -logcarat, data = dtrain)
logfull = lm(logprice ~. -price -carat, data = dtrain)
```

To choose between `price` or `logprice`, we must see how they behave under the assumptions of multiple regression. This can be done by looking at the `F-statistic` and `Multiple R-squared` from the `summary`, and generating Q-Q plots and plots for the standardized residuals vs. the fitted values of the model. We have here denoted the model `full` as the the model with response variable `price`, and the model `logfull` as the model with response variable `logprice`. 

KOMMENTAR: kan også se på boxcox(full,plotit=TRUE)

```{r,eval = TRUE}
summary(full)
summary(logfull)
```

```{r,eval = TRUE, out.width=c('50%', '50%'), fig.show='hold'}
library(ggplot2)
ggplot(full, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals full model",
       subtitle = deparse(full$call))
ggplot(full, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q full model", subtitle = deparse(full$call))
```

```{r,eval = TRUE, out.width=c('50%', '50%'), fig.show='hold'}
library(ggplot2)
ggplot(logfull, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals logfull model",
       subtitle = deparse(logfull$call))
ggplot(logfull, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q logfull model", subtitle = deparse(logfull$call))
```

We see from the information in the `summary` and the plots that `logprice` is preferred when doing inference about the predicted price of a diamond. We see that the regression is significant for both models, although `logfull` has a higher `F-statistic` and may therefore be preferred. Both models explain the variability in the data well, but again, `logfull` has a higher `Multiple R-squared` and is therefore preferred. The biggest difference between the models are the violatons of the model assumptions for multiple regression, in particular the residuals, seen from the plots. The `full` model has residuals that greatly vary with the fitted values of the response, breaking the assumption of constant variance for the residuals. In addition, the residuals can not be said to come from a normal distribution, as they diverge greatly from the plotted normal distribution in the Q-Q plot. This can to some degree also be said about `logfull`, as we see the distribution of the residuals deviate at the tails in the Q-Q plot. On the other hand, the residuals seem to have a more constant spread plotted against the fitted values. In total, we therefore prefer `logprice` as the response variable.

To plot the response `logprice` pairwise with the covariates `carat`, `logcarat`, `color`, `clarity` and `cut`, we can make a scatter plot.


```{r,eval = TRUE}
scatter_plot = dtrain[, c("logprice","carat", "logcarat", "color", "clarity", "cut")]
pairs(scatter_plot)
```

KOMMENTAR: Veldig vanskelig å se hvordan color, clarity og cut påvirker, mulig å lage enkelte scatterplots for hver variabel? Får ikke til med "plot(x,y)" funksjonen ..

Her er et slags scatterplot: 

```{r,eval=TRUE}
price=unlist(dtrain[,1])
logprice=unlist(dtrain[,2])
carat = unlist(dtrain[,3])
logcarat = unlist(dtrain[,4])
color = unlist(dtrain[,6])
clarity = unlist(dtrain[,6])
cut = unlist(dtrain[,5])

plot(logprice, carat, type = "p") 
plot(logprice, logcarat, type = "p") 
plot(logprice, cut, type = "p")
plot(logprice, color, type = "p")
plot(logprice, clarity, type = "p")
plot(logprice, cut, type = "p")
```

***
Use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$.

***
**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.

```{r,eval = TRUE}
fit = loess(logprice ~ carat, span = .2)
price1 = predict(fit, 1)
price1
```
***
**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?

If we choose $\beta_1=\beta_2 =0$ and $K_{i0} = 1$(or any other constant $k<0$), it will result in KNN-regression. 
The $\beta_0$ that is left will give the average value in the group, and a constant kernel will give the same weight to all observations. ...And these are the same principles that KNN-regression is based on. 

***
**Q4:** Describe how you can perform model selection in regression with AIC as criterion.

Model selection can be done by best subset model selection, which aims to produce a subset of the model that performs best according to some model choice criteria. The method goes through every possible combination of covariates $p \choose j$ for $j = 1,2, \ _\cdots \ ,p$, and chooses a model for each $j$ that has the lowest $SSR$. The resulting $p+1$ models (intercept included) are then evaluated with a chosen model choice criteria, e.g AIC.
$$
AIC = \frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)
$$
The AIC is computed for each of the $p+1$ models, and the model with lowest score is deemed as the best model. $d$ is here the number of predictors, otherwise known as the $p+1$ covariates (intercept included). We can then see how the AIC penalizes larger models.  
***
**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?
Using AIC for model selection we may get a reduced model with fewer covariates, in contrast to using cross-validation which only gives variations of the full model. The data is also divided into different sections (folds) when doing cross-validation, whereas in model selection with AIC (or other criteria) the whole training set is used for selecting the model.
***
**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.
```{r,eval=TRUE}
library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))
fit=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)
```
KOMMENTAR: Contrast?? Treatment?
We see from the `summary` of `fit` that the best model includes the covariates `logcarat`, `cut`, `color`, `clarity` and `xx`.
KOMMENTAR: What can we interpret from the model??
***
**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).  
```{r,eval=TRUE}
bss_prediction = predict.lm(fit, dtest)
bss_testMSE = mean((dtest$logprice - bss_prediction)^2)
```
Using the R code above, the test MSE for best subset selection is calculated to $\text{MSE}_{test} = `r I(bss_testMSE)`$. 

***
**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?

```{r,eval=TRUE}
model_matrix = model.matrix(logprice ~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data = dtrain)
dim = dim(model_matrix)
```

The model matrix for the above covariates has dimensions $(`r I(dim[1])` \times `r I(dim[2])`)$.

***
**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?

```{r,eval=TRUE}
library(glmnet)
cv.out = cv.glmnet(model_matrix, dtrain$logprice, alpha=1)
lambda_min = cv.out$lambda.min #Can also choose lamda.1se for one standard error rule
lasso <- glmnet(model_matrix, dtrain$logprice, alpha=1, lambda = lambda_min)
lasso_coeff = coef(lasso)
lasso_coeff
```

Fitting a lasso regression is done with `glmnet` by setting `alpha`$=1$. Here we have found the optimal regularization parameter $\lambda = `r I(lambda_min)`$ by cross-validation on the training set, which is then used to fit the lasso regression. We can see by the print-out above how the lasso shrinks some of the estimated coefficients towards zero, where the ones that are shrunk the most are least significant in the regression.

***
**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).

```{r,eval=TRUE}
model_matrix_test = model.matrix(logprice ~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data = dtest)
lasso_prediction = predict(lasso,s=lambda_min,newx=model_matrix_test)
lasso_testMSE = mean((dtest$logprice - lasso_prediction)^2)
```

Using the R code above, the test MSE for lasso regression is calculated to $\text{MSE}_{test} = `r I(lasso_testMSE)`$.

***
**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.

A greedy approach will take locally optimal choices in order to approximate a globally optimal solution. For a regression tree we want to construct a tree that minimizes the $RSS$ of the training set, but we must then compute the $RSS$ for each possible partition of the predictor space. A more computationally feasible strategy is using the greedy algorithm _recursive binary splitting_. Here we split the predictor space in two for each predictor and minimize the $RSS$ for the binary split, resulting in two branches for each predictor. This means taking a locally optimal choice, namely the binary split that minimizes the $RSS$ for a specific predictor, without concerning ourselves with the overall $RSS$. In order for a greedy approach to approximate the global optimum, the problem at hand should have an optimal substructure, meaning the global optimum contains the local optimums of each sub-problem.

***
**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.

A tree based method may be suitable as long as we can split the predictor space into sensible regions. In a binary split, the splitting condition may take on a true or false value. E.g for a numerical covariate, the splitting condition may be if the covariate is less than a certain value. Similarly for a categorical covariate (e.g `cut` for our model), we may split on the condition that the covariate takes on one or several of the categorical values or not. Here we may have interactions, e.g the condition that the `cut` of a diamond is <`Very good` or `Premium`> or not. This way, a regression tree may be suitable for both numerical and categorical covariates.  

***
**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.

```{r,eval=TRUE}
library(tree)
regtree = tree(logprice ~logcarat+cut+color+clarity+depth+table+xx+yy+zz-1, data = dtrain)
plot(regtree, type="uniform")
text(regtree)
```


***
**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

```{r,eval=TRUE}
tree_prediction = predict(regtree, newdata = dtest)
tree_testMSE = mean((dtest$logprice-tree_prediction)^2)
```

Using the R code above, the test MSE for a tree-based regression is calculated to $\text{MSE}_{test} = `r I(tree_testMSE)`$.

***
**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?  

Bagging is used to lower the variance of a predictor by taking an average of each bootstrap sample prediction. This is especially useful for predictors that have high variance in the first place, e.g decision trees. Bootstrapping is here used to generate $B$ trees from the $n$ observations of our sample. Bagging will perform poorly when there is a strong predictor in the dataset, since most of the trees generated will have similar splits for the first nodes. The trees then become correlated, and an average will not enhance the variance much. A solution to this is random forests. The motivation behind random forests is similar to bagging; we want to reduce the variance of our predictor. This is done by taking bootstrap samples of our observations and generating trees with a random subset of the predictors as split for each node. This is a contrast to bagging, where all predictors are considered for each split. The end result is a number of different trees (forest), and we can take a majority vote for classification problems and a mean for regression problems when using the forest on a dataset.

***
**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?

A parameter to be set in a random forest is the subset $m$ of all predictors $p$ being used in each split. This is constrained to be less than the number of predictors, so $m < p$ for each tree in the forest. Otherwise, the method would be identical to bagging. For regression trees a popular choice for $m$ is $p/3$, while for classification trees a popular choice is $\sqrt{p}$. The number of trees $B$ that our forest consists of should be large, in order to reduce the variability of the method.

***
**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?

In boosting, a single tree is grown sequentially from several previously fitted trees. A big difference from a random forest is that there is no use of bootstrapping when boosting, the whole dataset is used when fitting trees sequentially.

***
**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).

```{r,eval=TRUE}
library(randomForest)
rf = randomForest(logprice ~logcarat+cut+color+clarity+depth+table+xx+yy+zz-1, data=dtrain, mtry=3, ntree = 500, importance=TRUE)
```

We have 9 predictors (covariates) in our model, and following the typical choice of $p/3$ for each subset in a regression tree we get 3 predictors for each subset. This ensures that most of the trees aren't correlated. The number of trees in the forest is chosen to a fairly high number to ensure a lower variability from the resulting prediction.  

***
**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).

```{r,eval=TRUE}
rf_prediction = predict(rf, newdata = dtest)
rf_testMSE = mean((dtest$logprice-rf_prediction)^2)
varImpPlot(rf)
```

The plot for node impurity ranks the predictors based on the decrease in RSS due to a split for a predictor, where predictors resulting in a large decrease in RSS are ranked as important.

Using the R code above, the test MSE for a the random forest is calculated to $\text{MSE}_{test} = `r I(rf_testMSE)`$.

***
**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tab <- " 
| Method                | MSE                 |
|-----------------------|---------------------|
| Best subset selection | 0.00360             |
| Lasso                 | 0.00364             |
| Tree                  | 0.01715             |
| Random forest         | 0.00281             |
"
cat(tab)
```


# Problem 2: Unsupervised learning [3 points]
***
**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix ${\hat {\bf R}}$. 

The principal component scores are the $z_{1i}, ..., z_{ni}$ elements in the principal component vectors, where
$z_{in} = \phi_{11}x_{i1}+\phi_{21}x_{i1}+...+\phi_{p1}x_{ip}$

The direction of the first principal component is given by the first eigenvector(${\bf \phi$}) of the covariance matrix. The first eigenvector is the one with the largest eigenvalue. 

***
**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

In the plot we see the weights that the different genes are given in the first eigenvector ${\bf\phi_1}$. 

Where ${\bf \phi_1}=[\phi_{11}, \phi_{13},...,\phi_{1p}]$ and $\phi_{1i}$ are the different weights.

The Rank of a matrix centered by columns is $\leq(n-1,p)$, which gives our $\bf Z$ a rank 63. Then the sample covariance matrix should also have rank 63, which gives us a maximum of 63 principal components. ????

***
**Q23:** How many principal components are needed to explain 80% of the total variance in ${\bf Z}$? Why is `sum(pca$sdev^2)=p`?  

We can read from the summary that we need 32 PCA´s for the cumulative proportion of variance explained to be 80%.  

The PCA analysis gives 64 principal components that in total explain all the variability in the data. This means that the sum of the variance in the PCA equals the total variance of the data, sum(pca.sdev^2) =$\sigma_{tot}^2$. 
We know that the column standard deviations for $\bf Z$ are 1, which gives column variance equal 1. To get the total variance in the data we sum the variance for each column. 
In our case: $\sigma_{tot}^2=6830\cdot 1=6830=p$
***
**Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.


```{r}
library(ElemStatLearn)
X=t(nci) #n times p matrix
table(rownames(X))
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,14)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19
colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]
Z=scale(X)
pca=prcomp(Z)
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
plot(pca$x[,3],pca$x[,4],xlab="PC3",ylab="PC4",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")

summary(pca)
```

For the PC1/PC2 plot we observe that several groupings of the same cell types have been made. The leukimia cells are grouped together with the K562 cells, which intuitively makes sense. The MC57 cells are spread out together with the other breast cancer cells. This might mean that the two first principal component don't hold much relevant information for these cells. 
The UNKNOWN sample is placed quite close to the origin, together with many other different cancer tumors. This might mean that these pricinpal components do not hold enough useful information about these tumors to clearly tell them apart. 

In the PC3/PC4 plot we see some of the same groupings, eg. leukimia and colon. The breast cancer cells are still spread out and the UNKNOWN sample is still quite close to the origin. The MC57 are not present in this plot, telling us ?? 

***
**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 

In hierarchical clustering we define a measure of dissimilarity, often the euclidean distance. The euclidean distance refers to the distance beetween pairs of observations: 
$$||a-b||_2=\sqrt{\sum_i(a_i-b_i)^2}$$

The use of euclidean distance will cluster observations of similar magnitude together.

Linkage describes the concept of dissimilarity beetween groups of observations. In average linking the dissimilarity between two clusters, A and B, is given by the mean intercluster dissmilarity. All pairwise dissimilarities (in our case euclidean distances) between the observations in cluster A and B are computed, and the average is used. 
$L(A,B)=\frac{1}{n_An_B}\sum_{i=1}^{n_A}\sum_{j=1}^{n_B}D(x_{Ai},x_{Bj})$

***
**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?¨

```{r,eval = TRUE}
library(dendextend)
hc = hclust(dist(Z[,-1]), method = "average")
plot(hc, xlab = "", sub = "")
str(hc)
```

The K562 samples are placed close to the other Leukimia samples, and the MC57s close to other breast cancer samples. Their clusters are placed low in the dendogram, which tells us they were one of the first clusters to be made and that the samples have clear similarities beetween each other. 

The UNKNOWN sample is placed close to several different cancer types, which indicates that there is a group of cancers that have similar characteristics and that are difficult to tell apart based on the gene activity. This is supported by the plots from Q24, which also scatter these cancer types close to each other.
***
**Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on thefirst 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```
From "the curse of dimensonality", we have that the use of Euclidean distance in high dimensional spaces might be problematic. By reducing the dimensions in the problem from $p$ to 64 the use of this distance metric might/will make more sense. But we still have quite many dimensions?

Using the principal components enables us to make a heatmap of reasonable size and reduces the computation time for clustering.  Using the forst 64 principal components means using all of them, and by doing this we preserve all the variance in the data. 

The horisontal axis list the principal components. The vertical axis, left side, shows the dendogram for the clustering. Vertical axis, right side, are the observed cancer types. As the vertical axis "sorts" the cancer tumors based on similarity, we can quickly see what cancer tumors had similar gene activity. 
The colors in the pixel grid represent the changes in the expression of the cancer tumor. E.g. the leukimia tumors are given similar scores in PC1, and we can see that they must activate many of the same genes. In PC2 the same tumors are given more "neutral" colours, which tells is they are harder to tell apart from other cancer tumors baced on the variance captured in PC2. 

---
output:
  pdf_document: default
  html_document: default
---
# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```
***
**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

```{r,eval = TRUE}
library(plyr)
library(phonTools)
library(ggplot2)

summary(ctrain)

#make plotable vectors from the dataset: 
diabetes=unlist(ctrain[,1])
npreg=unlist(ctrain[,2])
glu=unlist(ctrain[,3])
bp=unlist(ctrain[,4])
skin=unlist(ctrain[,5])
bmi=unlist(ctrain[,6])
ped=unlist(ctrain[,7])
age=unlist(ctrain[,8])

#SCATTER PLOTS 
tot_scatter_plot = ctrain[, c(1:8)]
pairs(tot_scatter_plot)

plot(diabetes, ped)

#gludata = ctrain[, c(1,3)]
#glm.glu = glm(diabetes ~ ., data = ctrain[, c(1,3)], family = "binomial")
#summary(glm.glu)

plot2 = ggplot(ctrain, aes(x = ped, y = glu, color = diabetes)) + 
    geom_point(size = 2.5)
plot2

#fit1 = lm(diabetes ~ glu)
#plot(fit)
```

```{r,eval = TRUE}

#NPREG
#adapting pregnancydata to see if it is any use 
preg1=zeros(1:17)
preg0=zeros(1:17)

for (j in 1:17){
  for (i in 1:100){
    if (ctrain[i, 2]==j)
        preg1[j]=preg1[j]+1
    }
}# makes preg = [number of people with one pregnancy, number of people with 2 pregrancies etc...] when diabetes = 1

for (j in 1:17){
  for (i in 101:300){
    if (ctrain[i, 2]==j)
        preg0[j]=preg0[j]+1
    }
}# makes preg = [number of people with one pregnancy, number of people with 2 pregrancies etc...] when diabetes = 0


#GLU
glu1=ctrain[0:100,3] #gluv-erdier for dem med diabetes = 1
glu0=ctrain[101:300, 3] #glu-verdier for dem med diabetes = 0
glu1=sort(glu1)
glu0=sort(glu0)
glu0short = glu0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, glu1, type = "l")
lines(xaxis,glu0short,col="green")

#BLOOD PRESSURE 
bp1=ctrain[0:100,4]
bp0=ctrain[101:300, 4]
bp1=sort(bp1)
bp0=sort(bp0)
bp0short = bp0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, bp1, type = "l")
lines(xaxis,bp0short,col="green")

#SKIN 
skin1=ctrain[0:100,5]
skin0=ctrain[101:300, 5]
skin1=sort(skin1)
skin0=sort(skin0)
skin0short = skin0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, skin1, type = "l")
lines(xaxis,skin0short,col="green")

#BODY MASS INDEX 
bmi1=ctrain[0:100,6]
bmi0=ctrain[101:300, 6]
bmi1=sort(bmi1)
bmi0=sort(bmi0)
bmi0short = bmi0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, bmi1, type = "l")
lines(xaxis,bmi0short,col="green")

#DIABETES PEDRIGREE FUNCTION 
ped1=ctrain[0:100,7]
ped0=ctrain[101:300, 7]
ped1=sort(ped1)
ped0=sort(ped0)
ped0short = ped0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, ped1, type = "l")
lines(xaxis,ped0short,col="green")

#AGE 
age1=ctrain[0:100,7]
age0=ctrain[101:300, 7]
age1=sort(age1)
age0=sort(age0)
age0short = age0[c(TRUE, FALSE)] #tar ut annenhver verdi 

xaxis = c(1:100)

plot(xaxis, age1, type = "l")
lines(xaxis,age0short,col="green")


```


***
**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification
* one method from Module 8: Trees (and forests)
* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

```{r,eval = TRUE}
#npreg + glu + bp + skin + bmi + ped + age,
library(knitr)

#MODULE 4
#LOGISTIC REGRESSION
glm_diabetes = glm(diabetes ~ ., data = ctrain, family = "binomial")
summary(glm_diabetes)

probs_diabetes = predict(glm_diabetes, newdata = ctest, type = "response")

pred_diabetes = ifelse(probs_diabetes > 0.5, 1, 0)

predictions_diabetes = data.frame(probs_diabetes, pred_diabetes, ctest)
colnames(predictions_diabetes) = c("Estim. prob. of Y=1", "Predicted class", 
    "True class")
kable(head(predictions_diabetes), booktabs = TRUE)
conf_test=table(pred_diabetes, ctest[, 1]) #confusion matrix 

#testing misclass error
misclass_test = (conf_test[1,2]+conf_test[2,1])/(sum(conf_test))
misclass_test

#training misclass error (FUNGERER IKKE)
#diabetes_train_prob = glm_diabetes$fitted.values
#diabetes_train_pred = ifelse(diabetes_train_prob > 0.5, 1, 0)
#conf_train = table(diabetes_train_pred, ctrain[, 1])
#misclas_train = (232 - sum(diag(conf_train)))/232
#misclas_train
```
The logistic function: 
$p(X)=\frac{e^{\beta_0+\beta_1X_1+\cdot\cdot\cdot+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+\cdot\cdot\cdot+\beta_pX_p}}$
We fit the model using maximum likelihood. 
Assuming
 - observations that are independent of each other. 
 - little or no multicolliniearity among the independent variables. 
 - linearity of independent variables and log odds. 
 - large enough sample size. 
 
 The misclassification test error is 20%. 

```{r,eval = TRUE} 
 library(pROC)
diabetes_roc = roc(ctest[,1], probs_diabetes, legacy.axes = TRUE)
ggroc(diabetes_roc) + ggtitle("ROC curve") + annotate("text", x = 0.25, y = 0.3, 
    label = "AUC = 0.8451")
auc = diabetes_roc$auc
```

```{r,eval = TRUE} 
#MODULE 8
#TREES
library(tree)
diabetes.tree = tree(as.factor(diabetes) ~ ., data = ctrain, split = "gini", method = "class") #evt cross-entropy split? 
summary(diabetes.tree)
plot(diabetes.tree, type = "proportional")
text(diabetes.tree)

library(caret)
diabetes.pred = predict(diabetes.tree, newdata = ctest, type = "class")
confusionMatrix(diabetes.pred, reference = as.factor(ctest$diabetes))

#random forest  
library(randomForest)
set.seed(300)
rf = randomForest(as.factor(diabetes) ~ ., data = ctrain, mtry = 3, ntree = 500, importance = TRUE)
rf$confusion
1 - sum(diag(rf$confusion[1:2, 1:2]))/(sum(rf$confusion[1:2, 1:2]))
yhat.rf = predict(rf, newdata = ctest)
misclass.rf = table(yhat.rf, ctest$diabetes)
print(misclass.rf)
misclass = 1 - sum(diag(misclass.rf))/(sum(misclass.rf))
print(misclass)
varImpPlot(rf, pch = 20)
predrf = predict(rf, ctest, type = "prob")
testrfroc = roc(ctest$diabetes == "1", as.numeric(predrf[,2]))
ggroc(testrfroc) + ggtitle("ROC curve Random Forest") + annotate("text", x = 0.25, y = 0.3, 
    label = "AUC = 0.84")
auc = testrfroc$auc
```

Explain prediction and splitting criterion. 

Choosing random forest instead of bagging because we have a strong predictor in the dataset (glu). We don't want trees that are highly correlated, because this won't give us a large reduction in variance when averaging $\hat{f}(x)$
Choosing mtry = 3 beause $\sqrt{7}\approx 3$

Burde det prunes for å unngå overfitting? 

The misclassification rate is 22%. 

```{r,eval = TRUE} 
#SUPPORT VECTOR MACHINE 
library(e1071)
svmfit_kernel1 = svm(as.factor(diabetes) ~ ., data = ctrain, kernel = "radial", gamma = 1, 
    cost = 10, scale = FALSE)
#plot(svmfit_kernel1, ctrain, col = c("lightcoral", "lightgreen"))
summary(svmfit_kernel1)
#find optimal tuning parameter C 
set.seed(1)
CV_kernel = tune(svm, as.factor(diabetes) ~ ., data = ctrain, kernel = "radial", gamma = 1, 
    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(CV_kernel)

bestmod_kernel = CV_kernel$best.model
ypred_kernel = predict(bestmod_kernel, ctest)

conf_svm = table(predict = ypred_kernel, truth = ctest[, 1])
misclass_svm= 1 - sum(diag(conf_svm))/(sum(conf_svm))
misclass_svm
```

Probabilities for each class not given, therefore no ROC. STEMMER DETTE? 
The misclassification rate is 29%. 

ARTIFICIAL NEURAL NETWORKS
```{r,eval = TRUE} 
library(caret)
library(nnet)
#bruk av keras er utenfor pensum 
library(keras) 

#Parameter estimitation 
fitnnet = nnet(diabetes ~ ., data = ctrain, 
    linout = TRUE, size = 0, skip = TRUE, maxit = 1000)
cbind(fitnnet$wts)

#parameterestimation vs. network weights
fitlogist = glm(diabetes ~ npreg + glu + bp + skin + bmi + ped + age, 
    data = ctrain, family = binomial(link = "logit"))
fitnnet = nnet(diabetes ~ npreg + glu + bp + skin + bmi + ped + age, 
    data = ctrain, linout = FALSE, size = 0, skip = TRUE, maxit = 1000, 
    entropy = TRUE, Wts = fitlogist$coefficients + rnorm(8, 0, 0.1))
#plot(fitnnet)

#single hidden layer 
fitnnet = nnet(as.factor(diabetes) ~., size = 50, data = ctrain, lineout = TRUE, maxit = 1000)
pred = predict(fitnnet, newdata = ctest, type = "raw")

sqrt(mean((pred[, 1] - ctest[,1])^2))
mae = mean(abs(pred[, 1] - ctest[,1]))
mae

conf_nnet = table(predict = pred, truth = ctest[, 1])
misclass_nnet= 1 - sum(diag(conf_nnet))/(sum(conf_nnet))
misclass_nnet

nnetroc = roc(ctest$diabetes, as.numeric(pred))
auc(testrfroc)
plot(testrfroc)


# <!-- # using cross validation to find the best number of hidden nodes -->
# <!-- grid = c(5, 10, 15, 20, 25, 30, 50) -->
# <!-- train_targets = ctrain[,1] -->
# 
# <!-- k <- 4 -->
# <!-- set.seed(123) -->
# <!-- indices <- sample(1:nrow(ctrain)) -->
# <!-- folds <- cut(indices, breaks = k, labels = FALSE) -->
# <!-- # have now assigned the traning data to 4 diffeent folds all of the -->
# <!-- # same size (101) -->
# 
# <!-- resmat = matrix(NA, ncol = k, nrow = length(grid)) -->
# <!-- for (j in 1:k) { -->
# <!--     thistrain = (1:dim(ctrain)[1])[folds != j] -->
# <!--     thisvalid = (1:dim(ctrain)[1])[folds == j] -->
# <!--     mean <- apply(ctrain[thistrain, ], 2, mean) -->
# <!--     std <- apply(ctrain[thistrain, ], 2, sd) -->
# <!--     new <- scale(ctrain, center = mean, scale = std) -->
# <!--     for (i in 1:length(grid)) { -->
# <!--         thissize = grid[i] -->
# 
# <!--         fit = nnet(unlist(ctrain) ~ ., data = new, size = thissize, linout = TRUE, -->
# <!--             maxit = 5000) -->
# <!--         pred = predict(fit, newdata = new[thisvalid, ], type = "raw") -->
# <!--         resmat[i, j] = sum((pred[, 1] - train_targets[thisvalid])^2) -->
# <!--     } -->
# <!-- } -->
# <!-- mse = apply(resmat, 1, sum)/nrow(ctrain) -->
# <!-- #plot(grid, mse, type = "l") -->
# <!-- mse -->
```

The neural network model: 
$$\hat{y}_1(x_i)=\phi_0(w_o+w_1x_{i1}+\cdot\cdot\cdot+w_px_{ip})$$ where $\phi_0(x)=x$. 
Loss function: ?
$$J({\bf w})=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_1}({\bf x_i}))^2$$

Chosen tuning parameters and model selection: 

report (any) insight into the interpretation of the fitted model: 

Mislcass = 75% :S 
AUC = 0.7407




***
**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.
