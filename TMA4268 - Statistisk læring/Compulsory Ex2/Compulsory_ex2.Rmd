---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 2"
author: "Anders Fagerli, Rebecca Sandstø"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document
  #pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE,echo=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
```

# Problem 1: Regression [6 points]

```{r}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
```
***
**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

```{r,eval = TRUE}
full = lm(price ~. -logprice -logcarat, data = dtrain)
logfull = lm(logprice ~. -price -carat, data = dtrain)
```

To choose between `price` or `logprice`, we must see how they behave under the assumptions of multiple regression. This can be done by looking at the `F-statistic` and `Multiple R-squared` from the `summary`, and generating Q-Q plots and plots for the standardized residuals vs. the fitted values of the model. We have here denoted the model `full` as the the model with response variable `price`, and the model `logfull` as the model with response variable `logprice`.

```{r,eval = TRUE}
summary(full)
summary(logfull)
```

```{r,eval = TRUE, echo=FALSE, out.width=c('50%', '50%'), fig.show='hold'}
library(ggplot2)
ggplot(full, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals full model",
       subtitle = deparse(full$call))
ggplot(full, aes(sample = .stdresid)) +
  stat_qq(pch = 19) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals",
       title = "Normal Q-Q full model", subtitle = deparse(full$call))
```

```{r,eval = TRUE, echo=FALSE, out.width=c('50%', '50%'), fig.show='hold'}
library(ggplot2)
ggplot(logfull, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals logfull model",
       subtitle = deparse(logfull$call))
ggplot(logfull, aes(sample = .stdresid)) +
  stat_qq(pch = 19) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals",
       title = "Normal Q-Q logfull model", subtitle = deparse(logfull$call))
```

We see from the information in the `summary` and the plots that `logprice` is preferred when doing inference about the predicted price of a diamond. We see that the regression is significant for both models, although `logfull` has a higher `F-statistic` and may therefore be preferred. Both models explain the variability in the data well, but again, `logfull` has a higher `Multiple R-squared` and is therefore preferred. The biggest difference between the models are the violatons of the model assumptions for multiple regression, in particular the residuals, seen from the plots. The `full` model has residuals that greatly vary with the fitted values of the response, breaking the assumption of constant variance for the residuals. In addition, the residuals can not be said to come from a normal distribution, as they diverge greatly from the plotted normal distribution in the Q-Q plot. This can to some degree also be said about `logfull`, as we see the distribution of the residuals deviate at the tails in the Q-Q plot. On the other hand, the residuals seem to have a more constant spread plotted against the fitted values. In total, we therefore prefer `logprice` as the response variable.

The response `logprice` is plotted pairwise with the covariates `carat`, `logcarat`, `color`, `clarity` and `cut` below.

```{r,eval=TRUE, echo=FALSE}
price=unlist(dtrain[,"price"])
logprice=unlist(dtrain[,"logprice"])
carat = unlist(dtrain[,"carat"])
logcarat = unlist(dtrain[,"logcarat"])
color = unlist(dtrain[,"color"])
clarity = unlist(dtrain[,"clarity"])
cut = unlist(dtrain[,"cut"])
```

```{r,eval=TRUE, echo=FALSE, out.width=c('50%', '50%'), fig.show='hold'}
plot(logcarat, logprice, type = "p", xlab="logcarat", ylab="logprice")
plot(color, logprice, type = "p", xlab="color", ylab="logprice")
plot(cut, logprice, type = "p", xlab="cut", ylab="logprice")
plot(clarity, logprice, type = "p", xlab="clarity", ylab="logprice")
```

We see that the price of a diamond is fairly linear with its carat. We have here only plotted against `logcarat`, as our chosen response is `logprice`, but `carat` will naturally affect the response similarly (only in a logarithmic manner). For the categorical covariates we produce boxplots, where the median line gives an indication of the `logprice` for each category. All categories are plotted from worst to best in terms of quality. We see that `color` has a steady increase in `logprice` as we go from worst to best, but the other covariates seem to have an opposite response. This may indicate that quality diamonds in terms of `cut` and `clarity` have otherwise poor traits, or that this is simply the case for our sample. In total, it seems that the `carat` of a diamond is the strongest predictor when it comes to estimating the price.

***
Use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$.

***
**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.

```{r,eval = TRUE}
fit = loess(logprice ~ carat, span = .2)
predictedlogprice = predict(fit, 1)
predictedprice = 10^predictedlogprice
```

The predicted price of a diamond weighting 1 `carat` is $5100.67 \$$. 

***
**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?

If we choose $\beta_1=\beta_2 =0$ and $K_{i0} = 1$, it will result in KNN-regression.

***
**Q4:** Describe how you can perform model selection in regression with AIC as criterion.

Model selection can be done by best subset model selection, which aims to produce a subset of the model that performs best according to some model choice criteria. The method goes through every possible combination of covariates $p \choose j$ for $j = 1,2, \ _\cdots \ ,p$, and chooses a model for each $j$ that has the lowest $SSR$. The resulting $p+1$ models (intercept included) are then evaluated with a chosen model choice criteria, e.g AIC.
$$
AIC = \frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)
$$
The AIC is computed for each of the $p+1$ models, and the model with lowest score is deemed as the best model. $d$ is here the number of predictors, otherwise known as the $p+1$ covariates (intercept included). We can then see how the AIC penalizes larger models.

***
**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?

By using AIC for model selection we may get a reduced model with fewer covariates, in contrast to using cross-validation which only gives variations of the full model. The data is also divided into different sections (folds) when doing cross-validation, whereas in model selection with AIC (or other criteria) the whole training set is used for selecting the model.

***
**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.

```{r,eval=TRUE}
library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))
fit=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)
```

We see from the `summary` how `cut`, `color` and `clarity` are represented in the model. As these are categorical covariates, each level may influence the response differently. This can be seen from the estimates of each level, where e.g an ideal `cut` affects the `logprice` more than a good `cut`. We also see from the `summary` of `fit` that the best model includes the covariates `logcarat`, `cut`, `color`, `clarity` and `xx`.
$$
\hat{Y_i}=\hat\beta_0+\hat\beta_{logcarat}\cdot\text{logcarat}_i+\hat\beta_{cut}\cdot\text{cut}_i+\hat\beta_{color}\cdot\text{color}_i+\hat\beta_{clarity}\cdot\text{clarity}_i+\hat\beta_{xx}\cdot\text{xx}_i
$$
The estimated coefficients for `cut`, `color` and `clarity` will in this model depend on which value the covariates take. We can from the new model interpret which covariates and levels are significant in the regression, and from the estimates how they affect the response.

***
**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`). 

```{r,eval=TRUE}
bss_prediction = predict.lm(fit, dtest)
bss_testMSE = mean((dtest$logprice - bss_prediction)^2)
```

Using the R code above, the test MSE for best subset selection is calculated to $\text{MSE}_{test} = `r I(bss_testMSE)`$.

***
**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?

```{r,eval=TRUE}
model_matrix = model.matrix(logprice ~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data = dtrain)
dim = dim(model_matrix)
```

The model matrix for the above covariates has dimensions $(`r I(dim[1])` \times `r I(dim[2])`)$.

***
**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?

```{r,eval=TRUE}
library(glmnet)
cv.out = cv.glmnet(model_matrix, dtrain$logprice, alpha=1)
lambda_min = cv.out$lambda.min #Can also choose lamda.1se for one standard error rule
lasso <- glmnet(model_matrix, dtrain$logprice, alpha=1, lambda = lambda_min)
lasso_coeff = coef(lasso)
lasso_coeff
```

Fitting a lasso regression is done with `glmnet` by setting `alpha`$=1$. Here we have found the optimal regularization parameter $\lambda = `r I(lambda_min)`$ by cross-validation on the training set, which is then used to fit the lasso regression. We can see by the print-out above how the lasso shrinks some of the estimated coefficients towards zero, where the ones that are shrunk the most are least significant in the regression.

***
**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).

```{r,eval=TRUE}
model_matrix_test = model.matrix(logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1, data = dtest)
lasso_prediction = predict(lasso, s=lambda_min, newx=model_matrix_test)
lasso_testMSE = mean((dtest$logprice - lasso_prediction)^2)
```

Using the R code above, the test MSE for lasso regression is calculated to $\text{MSE}_{test} = `r I(lasso_testMSE)`$.

***
**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.

A greedy approach will take locally optimal choices in order to approximate a globally optimal solution. For a regression tree we want to construct a tree that minimizes the $RSS$ of the training set, but we must then compute the $RSS$ for each possible partition of the predictor space. A more computationally feasible strategy is using the greedy algorithm *recursive binary splitting*. Here we split the predictor space in two for a predictor and minimize the $RSS$ for the binary split, resulting in two branches for the predictor. This means taking a locally optimal choice, namely the binary split that minimizes the $RSS$ for a specific predictor, without concerning ourselves with the overall $RSS$. In order for a greedy approach to approximate the global optimum, the problem at hand should have an optimal substructure, meaning the global optimum contains the local optimums of each sub-problem.

***
**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.

A tree based method may be suitable as long as we can split the predictor space into sensible regions. In a binary split, the splitting condition may take on a true or false value. E.g for a numerical covariate, the splitting condition may be if the covariate is less than a certain value. Similarly for a categorical covariate (e.g `cut` for our model), we may split on the condition that the covariate takes on one or several of the categorical values or not. Here we may have interactions, e.g the condition that the `cut` of a diamond is <`Very good` or `Premium`> or not. This way, a regression tree may be suitable for both numerical and categorical covariates.  

***
**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on your findings.

```{r,eval=TRUE}
library(tree)
regtree = tree(logprice ~logcarat+cut+color+clarity+depth+table+xx+yy+zz-1, data = dtrain)
plot(regtree, type="uniform")
text(regtree)
```

From the fitted regression tree we can see which binary splits result in the locally lowest $RSS$. Interesting to note is that only two of the predictors are used, indicating that `xx` and `yy` are suitable predictors for our dataset.

***
**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).  

```{r,eval=TRUE}
tree_prediction = predict(regtree, newdata = dtest)
tree_testMSE = mean((dtest$logprice-tree_prediction)^2)
```

Using the R code above, the test MSE for a tree-based regression is calculated to $\text{MSE}_{test} = `r I(tree_testMSE)`$.

***
**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?  

Bagging is used to lower the variance of a predictor by taking an average of each bootstrap sample prediction. This is especially useful for predictors that have high variance in the first place, e.g decision trees. Bootstrapping is here used to generate $B$ trees from the $n$ observations of our sample. Bagging will perform poorly when there is a strong predictor in the dataset, since most of the trees generated will have similar splits for the first nodes. The trees then become correlated, and an average will not enhance the variance much. A solution to this is random forests. The motivation behind random forests is similar to bagging; we want to reduce the variance of our predictor. This is done by taking bootstrap samples of our observations and generating trees with a random subset of the predictors as split for each node. This is a contrast to bagging, where all predictors are considered for each split. The end result is a number of different trees (forest), and we can take a majority vote for classification problems and a mean for regression problems when using the forest on a dataset.

***
**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?

A parameter to be set in a random forest is the subset $m$ of all predictors $p$ being used in each split. This is constrained to be less than the number of predictors, so $m < p$ for each tree in the forest. Otherwise, the method would be identical to bagging. For regression trees a popular choice for $m$ is $p/3$, while for classification trees a popular choice is $\sqrt{p}$. The number of trees $B$ that our forest consists of should be large, in order to reduce the variability of the method.

***
**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?

In boosting, a single tree is grown sequentially from several previously fitted trees. A big difference from a random forest is that there is no use of bootstrapping when boosting, the whole dataset is used when fitting trees sequentially.

***
**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).

```{r,eval=TRUE}
library(randomForest)
rf = randomForest(logprice ~logcarat+cut+color+clarity+depth+table+xx+yy+zz-1, data=dtrain, mtry=3, ntree = 500, importance=TRUE)
```

We have 9 predictors (covariates) in our model, and following the typical choice of $p/3$ for each subset in a regression tree we get 3 predictors for each subset. This ensures that most of the trees aren't correlated. The number of trees in the forest is chosen to a fairly high number to ensure a lower variability from the resulting prediction.  

***
**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).

```{r,eval=TRUE}
rf_prediction = predict(rf, newdata = dtest)
rf_testMSE = mean((dtest$logprice-rf_prediction)^2)
varImpPlot(rf)
```

The plot for node impurity ranks the predictors based on the decrease in RSS due to a split for a predictor, where predictors resulting in a large decrease in RSS are ranked as important. The plot based on randomization will test the predictive power of each predictor, one at a time, and rank the importance based on the OOB error. From the plots we can see which predictors are deemed important, and notice that the two plots are in disagreement. This may mean that several of the predictors are suitable for our dataset.

Using the R code above, the test MSE for a the random forest is calculated to $\text{MSE}_{test} = `r I(rf_testMSE)`$.

***
**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tab <- "
| Method                | MSE                 |
|-----------------------|---------------------|
| Best subset selection | 0.00360             |
| Lasso                 | 0.00364             |
| Tree                  | 0.01715             |
| Random forest         | 0.00281             |
"
cat(tab)
```

By calculating the MSE of each method, we see that the random forest has best performance on the test set. In terms of interpretation, the tree-based methods give good visual insight into how the price is predicted by the covariates. It does not necessarly show the relationship between the covariates and the response, where the other methods have the benefit of giving clear weights (coefficient values) to each covariate, giving us a sense of the relationship between each covariate and the response. 

# Problem 2: Unsupervised learning [3 points]
***
**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix ${\hat {\bf R}}$.

*Remark*: We will here denote a principal component as ${\bf Z}_m$, not to be confused with the standardized design matrix in the task.

The principal component scores are the principal components projected onto the data, ${\bf Z}_m{\bf X}_i=z_{1}x_{i1}+z_{2}x_{i2}+\cdots+z_{n}x_{ip}$, where each principal component ${\bf Z}_m$ is a weighted sum of the covariates ${\bf Z}_m=\sum_{j=1}^{p}\phi_{jm}X_j$ subject to $\sum_{j=1}^{p}\phi_{jm}^2=1$. The weights $\boldsymbol{\phi}_m$ are given by the eigenvectors of the estimated covariance matrix ${\hat {\bf R}}$.

***
**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

In the plot we see the weights that the different genes are given by the first eigenvector ${\boldsymbol\phi}_1$, and thus also how the first principal component ${\bf Z}_1$ weights the genes. The rank of the design matrix can at maximum be 64, as we only have $n=64$ samples, so the maximum amount of eigenvectors (orthogonal to eachother) is then 64. The rank of a matrix centered by columns is $\leq(n-1,p)$, which gives our $\bf Z$ a rank 63. Then the sample covariance matrix will also have rank 63, which should give us a maximum of 63 principal components. Principal component ${\bf Z}_{64}$ can be seen from the `summary` below to equal 0.

***
**Q23:** How many principal components are needed to explain 80% of the total variance in ${\bf Z}$? Why is `sum(pca$sdev^2)=p`?  

We can read from the `summary` that we need 32 PCs for the cumulative proportion of variance explained to be 80%.  

The PCA analysis gives 64 principal components that in total explain all the variability in the data. This means that the sum of the variance in the PCs equals the total variance of the data, sum(pca.sdev^2) =$\text{total variance}$.
We know that the column standard deviations for the standardized design matrix $\bf Z$ are 1, which gives column variance equal 1. To get the total variance in the data we sum the variance for each column.

In our case: $\text{total variance}=6830\cdot 1=p$

***
**Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.

```{r}
library(ElemStatLearn)
X=t(nci) #n times p matrix
table(rownames(X))
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,14)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])]
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
#legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")
```

For the PC1/PC2 plot we observe that several groupings of the same cell types have been made. This indicates that the analysis has suceeded in placing cancer tumours that have similar gene activity together. The K562 cells are grouped together with the leukemia samples, as K562 is a leukemia cell. Similarly, the MC57 cells are spread out together with the other breast cancer samples. The UNKNOWN sample is placed together in a grouping with many other different cancer tumors, indicating that it may be one of these. The plot illustrates the abilities of principal components, where we now can use only two vectors to explain the data, in comparison to using all the 6830 genes. The legend in the R code above can be uncommented to see which markers correspond to which cells.

```{r,eval=TRUE}
plot(pca$x[,3],pca$x[,4],xlab="PC3",ylab="PC4",pch=pchsamples,col=colsamples)
#legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)
summary(pca)
```

In the PC3/PC4 plot we see some of the same groupings, e.g the K562 cells are still grouped with leukemia samples, although the samples are not as grouped this time. A big difference from the previous plot is where the groupings are placed, and the groupings for UNKNOWN and MC57. Because the principal components are uncorrelated, but still give us the same tendencies, we will mostly see the same groupings but with different scores. 

***
**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering.

In hierarchical clustering we define a measure of dissimilarity, often the euclidean distance. The euclidean distance refers to the distance beetween pairs of observations:

$$
||a-b||_2=\sqrt{\sum_i(a_i-b_i)^2}
$$
The use of euclidean distance will cluster observations of similar magnitude together.
Linkage describes the concept of dissimilarity beetween groups of observations. In average linking the dissimilarity between two clusters, A and B, is given by the mean intercluster dissmilarity. All pairwise dissimilarities (in our case euclidean distances) between the observations in cluster A and B are computed, and the average is used.
$L(A,B)=\frac{1}{n_An_B}\sum_{i=1}^{n_A}\sum_{j=1}^{n_B}D(x_{Ai},x_{Bj})$

***
**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?¨

```{r,eval = TRUE}
hc = hclust(dist(Z[,-1]), method = "average")
plot(hc, xlab = "", sub = "")
str(hc)
```

The K562 samples are placed close to the other leukemia samples, and the MC57s close to other breast cancer samples. Their clusters are placed low in the dendogram, which tells us they were one of the first clusters to be made and that the samples have clear similarities beetween each other.

The UNKNOWN sample ends up beeing clustered together with several different cancer types, which indicates that there is a group of cancers that have similar characteristics and that are difficult to tell apart based on the gene activity. This is supported by the plots from **Q24**, which also scatter these cancer types close to each other.

***
**Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on thefirst 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

```{r}
library(pheatmap)
npcs=64
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```

Using the principal components enables us to make a heatmap of reasonable size and reduces the computation time for clustering. Using the first 64 principal components means using all of them, and by doing this we preserve 100% of the variance in the data.

The horisontal axis list the principal components. The vertical axis, left side, shows the dendogram for the clustering. Because we have preserved all the variance in the data, this dendogram should be the same as in **Q26**. Vertical axis, right side, are the observed cancer types. As the vertical axis "sorts" the cancer tumors based on similarity, we can quickly see what cancer tumors had similar gene activity.
The colors in the pixel grid represent the changes in the expression of the cancer tumor. E.g. the leukemia tumors are given similar values in PC1, and we can see that they must activate many of the same genes. In PC2 the same tumors are given more "neutral" colours, which tells us they are harder to tell apart from other cancer tumors baced on the variance captured in PC2.

From "the curse of dimensonality", we have that the use of Euclidean distance in high dimensional spaces might be problematic. By reducing the dimensions in the problem from $p$ to 64 the use of this distance metric might make more sense. But we still have quite many dimensions, so in our case this will not make much difference. 

# Problem 3: Flying solo with diabetes data [6 points]

```{r,eval=TRUE}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
```
***
**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

```{r,eval = TRUE,out.width=c('100%'), fig.show='hold'}
library(ggplot2)
library(GGally)
ggpairs(ctrain, ggplot2::aes(color=as.factor(ctrain$diabetes)),
        lower = list(continuous = wrap("points", alpha = 0.3, size=0.2)))
```

The pairs-plot above shows the relationship between the covariates and the response. From this we can see which covariates are more significant than others in predicting diabetes, and which covariates are correlated. E.g `skin` and `bmi` can be seen to have a linear relationship, so one would expect these to affect the response similarly. Otherwise, it is difficult to see any clear relationships between the covariates, which is confirmed by the calculated correlations right of the diagonal. In the leftmost column we see how diabetes is classified by the individual covariates, where `glu` and `bmi` seem to be the most significant covariates. Again, we see no clear relationships between the response and other covariates, as people with diabetes seem to have similar traits to those without diabetes. In total, this will make classification more difficult. To the left of the diagonal are scatterplots for different combinations of covariates, with the respective classes plottet. Here we can see which combinations seperate the two classes best. By visual inspection, `glu` and `bmi` seem to seperate the two classes best, supporting our previous statement that they may be the most significant covariates.

Some classification methods assume that the data comes from specific probability distributions, e.g LDA assumes the normal distribution. It may therefore be valuable to investigate if there are any known distributions present in our data. This can be done with a *Cullen and Frey graph*, from the `fitdistrplus` package, on each covariate separately. To investigate if a covariate is normally distributed, we can also compute the $p$-value of the Anderson-Darling test. An example is given in the R code below with `glu` and `bmi`.

```{r,eval=TRUE, out.width='100%'}
#Cullen and Frey graph
library(fitdistrplus)
vec.glu = c(ctrain["glu"]$glu)
descdist(vec.glu)
#Anderson-Darling test
library(nortest)
vec.bmi = c(ctrain["bmi"]$bmi)
ad.test(vec.bmi)
```

Repeating this for all covariates, we find that none are normally distributed.

***
**Q29:** Use different methods to analyse the data. In particular use

* one method from Module 4: Classification
* one method from Module 8: Trees (and forests)
* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

***
**Module 4: Classification**

We have several methods to choose from:

* LDA
* QDA
* Logistic regression
* KNN

As the data isn't normally distributed and the classes aren't well separated, the performance of LDA and QDA falls quickly. We therefore choose either logistic regression or KNN, and we will here look at logistic regression.

```{r,eval = TRUE}
#Model selection using best subset selection
df = as.data.frame(within(ctrain, {
  y=diabetes
  diabetes=NULL
}))
fit = bestglm(Xy=df, familiy=binomial, IC="AIC")$BestModel
fit$coefficients
```

The logistic function takes the form

$$
p_{i}=\frac{e^{\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip}}}{1+e^{\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip}}}
$$

where $p_i$ is the probability of classifying to `diabetes`$=1$ and $(1-p_i)$ to `diabetes`$=0$, and $\hat{\boldsymbol\beta}$ is found by maximizing the likelihood function. This classification method will only work on problems with two classes, and we assume that the classification $Y_i$ is Bernoulli distributed. In addition, we assume:

* Independent observations
* Little to no multicollinearity among independent variables
* Linearity between independent variables and log odds
* Large enough sample size

Using best subset selection, we may produce the combination of covariates that gives the best score according to some criteria, here the AIC. The resulting model consists of `npreg`, `glu`, `bmi`, `ped` and `age`, where we can interpret the importance of each covariate from print-out of the estimated coefficients above.

We can then test our new model on the test set.

```{r,eval=TRUE}
fit.probabilities = predict(fit, newdata = ctest, type = "response")
fit.predictions = ifelse(fit.probabilities > 0.5, 1, 0)
fit.confTable = table(fit.predictions, ctest[,1])
fit.error = (fit.confTable[1,2]+fit.confTable[2,1])/sum(fit.confTable)
```

Using the confusion matrix, the test error is calculated to
$$
\text{Test error} = \frac{31+15}{232} = 0.198
$$
The misclassification rate is thus $19.8\%$. The ROC and AUC are given below.

```{r,eval = TRUE, out.width='100%'}
library(pROC)
fit.roc = roc(ctest[,1], fit.probabilities, legacy.axes = TRUE)
auc = fit.roc$auc
ggroc(fit.roc) + ggtitle("ROC curve") + annotate("text", x = 0.25, y = 0.3,
    label = "AUC = 0.8547")
```

***
**Module 8: Trees**

A weakness with trees is their high variance. This can be improved by bagging, but in our dataset we have a strong predictor, `glu`. The result of bagging will then result in similar trees with the same predictor at the top, and the trees will then be correlated. For this reason, we will instead make a random forest to account for the strong predictor.

```{r,eval = TRUE}
library(randomForest)
set.seed(300)
rf = randomForest(as.factor(diabetes) ~., data=ctrain, mtry=3, ntree = 500, importance=TRUE)
```

We use the same model as when bagging,

$$ \hat{f}_{avg}({\bf x})=\frac{1}{B}\sum_{b=1}^B \hat{f}_b({\bf x}),$$
but choose to split randomly with a subset of the predictors, instead of using a splitting criteria. The parameter $B$ is chosen by bootstrapping. A tuning parameter in the random forest is the number predictors to choose from in each split, here chosen to `mtry`$=3$. When the predictors are correlated, the number of predictors to split on should be relatively low. From the pairs-plot in **Q28** we see that we don't have very correlated predictors, so we choose the typical amount of $\sqrt{p} = \sqrt{7} \approx 3$, where $p$ is the number of predictors. The number of trees to generate, `ntree`, is set to 500 in order to average over enough trees so the variance lowers.

```{r,eval=TRUE}
rf.prediction = predict(rf, newdata = ctest)
rf.confTable = table(rf.prediction, ctest$diabetes)
rf.error = (rf.confTable[1,2]+rf.confTable[2,1])/sum(rf.confTable)
```

Using the confusion matrix, the test error is calculated to
$$
\text{Test error} = \frac{23+30}{232} = 0.228
$$
The misclassification rate is thus $22.8\%$. The ROC and AUC are given below.
```{r,eval=TRUE, out.width='100%'}
rf.probabilities = predict(rf, ctest, type = "prob")
rf.roc = roc(ctest$diabetes == "1", rf.probabilities[,2])
rf.auc = rf.roc$auc
ggroc(fit.roc) + ggtitle("ROC curve") + annotate("text", x = 0.25, y = 0.3,
    label = "AUC = 0.8384")
```

***
**Module 9: Support vector machines**

A difficult part of using support vector machines for our dataset is choosing which kernel to base the decision boundry on. Usually, a visual inspection with a scatterplot will give an indication as to which boundry seems reasonable, but this is hard to visualize for models with $p>3$ predictors. Our approach is therefore testing out different kernels, and comparing the resulting misclassification rate from each model.

For a support vector *classifier*, we choose a linear decision boundry, where we wish to maximize the margin $M$ with respect to the coefficients $\boldsymbol{\beta}$ and slack variables $\epsilon_i$, with the constraints $\sum_{j=1}^{p}\beta_{j}^2=1$ and $\sum_{i=1}^{n}\epsilon_{i}\leq C$. We get the resulting classifier $f(\boldsymbol{x})=\beta_0+\beta_1x_1+\cdots+\beta_px_p$, where the sign of $f(\boldsymbol{x})$ determines the class. $C$ is here a tuning parameter, restricting the number of training observations that can be on the wrong side of the decision boundry (hyperplane). Choosing the value of $C$ is a trade-off between acceptable bias and variance, where a low value will allow fewer violations (less bias) and a high value will allow many violations (less variation). This tuning parameter is normally chosen by cross-validation.

Extending to support vector *machines*, we choose a non-linear decision boundry. This boundry is set by choosing a specific *kernel*, $K(\boldsymbol{x},\boldsymbol{x}_i)$, giving e.g a radial or polynomial boundry. The resulting classifier is $f({\bf x}_i)=\beta_0 +\sum_{l\in \cal{S}} \alpha_l K({\bf x}_i,{\bf x}_l)$, where a positive value for $f({\bf x}_i)$ classifies to one class, and a negative value classifies to the other.

The models above do not assume anything in particular about the data, as the methods are moreso a product of computer science than statistics.

The R code below tests three different models, with linear, radial and polynomial kernels. The tuning parameter $C$ is chosen by cross-validation for each model, where we choose from the values 0.001, 0.01, 0.1, 1, 5, 10 and 100.

```{r,eval = TRUE}
library(e1071)
#Linear boundry
svmlinear.cv = tune(svm, as.factor(diabetes) ~ ., data = ctrain, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
svmlinear.model = svmlinear.cv$best.model
svmlinear.prediction = predict(svmlinear.model, newdata=ctest)
svmlinear.confTable = table(svmlinear.prediction, ctest$diabetes)
svmlinear.error = (svmlinear.confTable[1,2]+svmlinear.confTable[2,1])/sum(svmlinear.confTable)
#Radial boundry
svmradial.cv = tune(svm, as.factor(diabetes) ~ ., data = ctrain, kernel = "radial", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
svmradial.model = svmradial.cv$best.model
svmradial.prediction = predict(svmradial.model, newdata=ctest)
svmradial.confTable = table(svmradial.prediction, ctest$diabetes)
svmradial.error = (svmradial.confTable[1,2]+svmradial.confTable[2,1])/sum(svmradial.confTable)
#Polynomial boundry
svmpoly.cv = tune(svm, as.factor(diabetes) ~ ., data = ctrain, kernel = "polynomial", degree = 3, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
svmpoly.model = svmpoly.cv$best.model
svmpoly.prediction = predict(svmpoly.model, newdata=ctest)
svmpoly.confTable = table(svmpoly.prediction, ctest$diabetes)
svmpoly.error = (svmpoly.confTable[1,2]+svmpoly.confTable[2,1])/sum(svmpoly.confTable)
```

We get the following results from the different classifiers:

```{r table4, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tab <- "
| Kernel    | Misclassification rate  |
|-----------|-------------------------|
| Linear    | 0.224                   |
| Radial    | 0.246                   |
| Polynomial| 0.280                   |
"
cat(tab)
```

We see that a linear decision boundry produces the smallest misclassification rate, so it seems this kernel fits our data best. As we do not get class probabilities using this method, we cannot produce a ROC curve.

***
**Module 11: Neural networks**

Firstly, we use cross validation to find the number of hidden nodes that will give us the best model.

```{r,eval = TRUE, trace=FALSE} 
library(caret)
library(nnet)
library(keras) 

org_train <- ctrain
train_data <- ctrain
test_data <- ctest
diabetes=ctrain$diabetes

#normalizing the data
org_train = ctrain
mean <- apply(ctrain, 2, mean)
mean[1]=0
std <- apply(ctrain, 2, sd)
std[1]=1
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)

#using CV to find best number of hidden nodes
grid = c(1, 2, 3, 4, 5) 
k <- 4
set.seed(123)
indices <- sample(1:nrow(ctrain))
folds <- cut(indices, breaks = k, labels = FALSE)

resmat = matrix(NA, ncol = k, nrow = length(grid))
for (j in 1:k) {
    thistrain = (1:dim(train_data)[1])[folds != j]
    thisvalid = (1:dim(train_data)[1])[folds == j]
    mean <- apply(org_train[thistrain, ], 2, mean)
    mean[1]=0
    std <- apply(org_train[thistrain, ], 2, sd)
    std[1]=1
    new <- scale(org_train, center = mean, scale = std)
    
    for (i in 1:length(grid)) {
        thissize = grid[i]
        
        fit = nnet(diabetes ~ ., data = train_data, size = thissize, linout = FALSE,
            maxit = 5000, trace = FALSE, entropy = TRUE, abstol = 1.0e-12, redtol = 1.0e-12) 
        pred = predict(fit, newdata = new[thisvalid, ], type = "raw")
        predfactor =ifelse(pred<0.5, 0, 1)
        resmat[i, j] = sum((predfactor[, 1]==diabetes[thisvalid]))
    }
}
misclass_cv = 1-apply(resmat, 1, sum)/nrow(train_data)
plot(grid, misclass_cv, type = "l")
```

We find the lowest misclassification rate when we use 4 nodes, and use this when fitting the model with the full test set. 

```{r,eval = TRUE} 
org_train = ctrain
mean <- apply(ctrain, 2, mean)
mean[1]=0
std <- apply(ctrain, 2, sd)
std[1]=1
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)

fitnnet = nnet(diabetes ~., size = 4, data = train_data, linout = FALSE, maxit = 5000, trace=FALSE, entropy = TRUE, abstol = 1.0e-12, redtol = 1.0e-12)
```

In `nnet` we can only have one hidden layer, and for single hidden layer feeedforward networks we have the model: 
$$
\hat{y}_c({\bf x})=\phi_o(\beta_{0c}+\sum_{m=1}^{M}\beta _{mc}z_m)=\phi_o(\beta_ {0c}+\sum_{m=1}^{M}\beta_{mc}\phi_h(\alpha_{0m} +\sum_{j=1}^{p}\alpha_{jm}x_j)
$$
where $\phi_0(x)=x$. 

By default, we have sigmoid as output layer activation: $$g(a)=\frac{1}{1+e^{-a}}$$
We set `linout` = FALSE because we want the output as a probability.

With out given $C=2$ classification problem, we use the binary cross-entropy loss function: 
$$
J({\bf \theta})=-\frac{1}{n}\sum_{i=1}^{n}(y_iln(\hat{y}_1({\bf x}_i)) + (1-y_i)ln(1-\hat{y}_1({\bf x}_i))
$$
where $\hat{y_1}({\bf x_i})$ is the output from the sigmoid output node and $y_i$ is the 0/1 observed class. 
We estimate det unknown parameters $\theta$ by minimizing the loss function $J(\theta)$, and `nnet` uses the bfgs method to achieve this. 
Our tuning parameters are: $\alpha_{jm}$ an $\beta_{mc}$.
When using neural networks we don't make any assumptions about the data.

```{r,eval = TRUE} 
prednnet = predict(fitnnet, newdata = test_data, type = "class")
prednnet_probabilities = predict(fitnnet, newdata = test_data, type = "raw")
``` 

We make a confusion matrix and calculate the misclassification rate:

```{r,eval = TRUE} 
#accuracy
conf_nnet = table(prednnet, truth = ctest[, 1])
misclass_nnet = 1-sum(diag(conf_nnet)/(sum(conf_nnet)))
```
$$
\text{Test error} = \frac{24+30}{232} = 0.233
$$
The misclassification rate is thus $22.8\%$. The ROC and AUC are given below.

```{r,eval = TRUE} 
#ROC and AUC
nnetroc = roc(test_data[,1], prednnet_probabilities)
nnet.auc = nnetroc$auc
ggroc(fit.roc) + ggtitle("ROC curve") + annotate("text", x = 0.25, y = 0.3,
    label = "AUC = 0.824")
```

***
**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

```{r table3, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tab <- "
| Method                 | Misclassification rate  |
|------------------------|-------------------------|
| Logistic regression    | 0.198                   |
| Trees                  | 0.228                   |
| Support vector machines| 0.224                   |
| Neural networks        | 0.233                   |
"
cat(tab)
```

In total, we see that logistic regression produces the lowest misclassification rate for our dataset, and is therefore the winning method. In addition, it has a short training time and is easier to interpret than some of the other methods. As previously mentioned, logistic regression is suited when we don't have clear class boundries, which we can see is the case from the scatterplots in **Q28**. This may be the reason it performs better than e.g support vector machines, which performance is dependent on these boundries. 

